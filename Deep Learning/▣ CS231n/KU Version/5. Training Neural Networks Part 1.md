## 5. Training Neural Networks Part 1 ##

※ 출처 : http://dsba.korea.ac.kr/wp/?page_id=167

※ 영상 : [링크](https://www.youtube.com/watch?v=a8jEXBAhRXI&list=PLetSlH8YjIfXMONyPC1t3uuDlc1Mc5F1A&index=5)

※ 작성자 : 김진솔

![1](https://i.imgur.com/xMR853Z.png)

지난 시간에는 add, max, multiply gate에서의 back-propagation과 sigmoid 자체 back-propagation의 결과를 구하여 add, max, multiply의 조합으로 sigmoid 를 만들었을 때의 back-propagation 결과와 sigmoid 자체 back-propagation 결과가 같음을 확인하였습니다.

※ 위의 sigmoid 미분 식에서 음의 부호가 중간에 생략됨. 결과는 맞음

![2](https://i.imgur.com/xCeE0Zy.png)

네트워크를 학습하다 보면 이전 layer의 한 줄기가 아닌 여러 줄기에서 뻗어나온 back-propagation들의 gradient가 합쳐지게 됩니다.

이러한 경우에 gradient들이 전부 더해져서 들어오게 됨을 학습하였습니다. 

더해서 들어오는 것을 학습하게 되면 ResNet 같은 구조에서 Skip connection에서 gradient가 결국 +(add)로 올라가서 합쳐짐을 알 수 있었습니다. 

bottleneck 구조에서 또한 dimension이 달라지더라도 1 x 1 convolution & stride 2로 확장하는 구조이기 때문에 back-propagation 시 gradient가 상위 layer로 올라가서 add 되는 것을 알 수 있었습니다.

![3](https://i.imgur.com/6DxAlvf.png)

요즘 모델의 추세는 FC를 많이 만들기 보다는 <span style="background-color: #FFFF00">CNN 에서 FC 보다 3x3 conv를 많이 사용하고 마지막에 GAP(Global Average Pooling)을 하는 경향</span>이 있습니다.

Weakly Supervised Learning을 할 때 Max pooling 보다는 GAP를 하면 더 좋은 결론이 나온다는 것이 알려져 있습니다.

![4](https://i.imgur.com/E55ewed.png)

back-propagation에 대한 수식을 상세히 보면 위와 같습니다. 위 수식에서 사용된 g(x)는 sigmoid가 사용되었고 cost function은 MSE가 사용되었다고 가정하고 수식을 전개하면 위와 같습니다. 

※ g(x) 는 sigmoid 함수로 미분 방법은 위에서 참조 바랍니다.

![5](https://i.imgur.com/ypkyMUn.png)

이전 슬라이드에서는 W2, b2에 대하여 미분을 하였다면 이번에는 W1, b1으로 미분한 결과 입니다. 이전 슬라이드 결과와 유사하게 결국에는 δ 형태로 묶을 수 있습니다. 

![6](https://i.imgur.com/PgOVSMA.png)

즉 위와 같이 chain rule과 같아지게 됩니다. 따라서 Add/Max/Multiply gate를 이용하여 구성한 Network에서의 back-propagation 과 결국 같은 값을 가지게 됩니다. 

![7](https://i.imgur.com/NJexDol.png)

Network 구조에 대하여 잠시 살펴 보면 FC(Fully Connected)가 있을 것이고 LC(Locally Connected)가 있을 것입니다. Convolution 연산은 FC/LC 측면에서 바라보면 LC에 해당하고 특히 weight를 공유하므로 shared-weight local 구조라고 할 수 있습니다.

![8](https://i.imgur.com/fAtnLCw.png)

CNN은 결국 Shared weight를 사용하고 Shared weight들에 대하여 gradient가 다 합쳐지는 것이라고 보면 됩니다.

![9](https://i.imgur.com/Ldf9m3L.png)

사용하고 싶은 데이터 (your data)가 있고, ImageNet 데이터를 이용하여 Pre-Train 된 데이터가 있을 때, 이 데이터를 초기 조건으로 사용하고 사용자가 사용할 데이터를 fine tuning 하여 Transfer Learning으로 사용할 수 있습니다.

 ImageNet 같은 유명한 모델들은 코드들을 구할 수 있습니다. 


> Transfer learning을 할 때 나의 데이터의 양이 많지 않다면, 끝단을 수정하고, 데이터의 양이 어느 정도 있다면 중간 부분을 수정하여 training을 하면 더 좋다고 알려져 있습니다. (무슨말인지...)

Transfer Learning 시 고려할 점은 다음과 같습니다.

1) Pre-train data와 추가할 data의 이질성이 심한 경우 사용을 자제 해야 한다.

2) Fine Tuning을 할 때 사용하는 데이터의 양에 따라 사용방법을 다르게 해야한다. (데이터의 양은 상대적임)

![10](https://i.imgur.com/IS05nY2.png)

CNN 에서는 Convolution연산과 Activation 연산이 하나의 집합이 되고 그 다음 Pooling이 일어나게 됩니다. 이렇게 만들어진 집합을 반복하게 됩니다. 그리고 마지막으로 FC를 적용하게 됩니다. 그리고 앞에서 언급한 바와 같이 최근 추세는 다음과 같습니다.

① 3 x 3 필터 여러번 사용 (큰 사이즈 필터 사용 안함)

② FC를 적게 사용하며 마지막 Conv 에서는 GAP을 사용

![11](https://i.imgur.com/0Jt82tt.png)

Activation Function에 대하여 알아보겠습니다. 한 layer의 결과가 activation function으로 들어가게 됩니다. 이 때 Σw*x + b의 형태가 선형 조합이기 때문에 비선형 조합으로 만들어 주어야 합니다.  

![12](https://i.imgur.com/UYDlgxo.png)

만약 Activation function을 Linear function을 사용한다면 layer을 계속 쌓더라도 선형 형태가 되어 의미가 없어 집니다.

![13](https://i.imgur.com/6j9wwdl.png)

그래서 통상적으로 Activation function은 Non-Linear function을 사용하게 됩니다. 

![14](https://i.imgur.com/ZNWZpF4.png)

Activation function은 TF 에서 다양하게 제공하고 있습니다.

![15](https://i.imgur.com/odeGcj5.png)

먼저 Sigmoid function 부터 살펴보겠습니다.

① 입력 값이 일정 범위의 safety zone을 넘어가게 되면 0 또는 1로 수렴하게 되고 gradient(경사값) 또한 0으로 수렴해 버리게 됩니다. 양 끝이 평평해 지기 때문입니다.

② sigmoid function의 범위가 [0, 1]입니다. 이 때문에 output의 중앙값이 0이 아니게 됩니다. 0을 기준으로 데이터가 분포하게 되었을 때가 이상적인데 Sigmoid에서는 하나의 단점이 됩니다.

③ Relu와 비교해 보았을 때, exp() 연산에 많은 cost가 듭니다.

![16](https://i.imgur.com/OVwpK6L.png) 

파란색 부분이 safety zone 입니다. safety zone을 넘어서게 되면 gradient 자체가 0으로 수렴하게 됩니다. 

![17](https://i.imgur.com/VjHPEgO.png)

Sigmoid가 Non-zero centered 분포를 가지게 되는 문제는 만약 input neuron이 항상 positive 라면 w에 대한 gradient가 항상 positive 또는 negative가 되게 학습이 잘 안될 수 있습니다. 학습하기 좋으려면 cost function이 minimize 되는 지점을 다양한 gradient 크기로 찾아가야 하는데 항상 positive/negative 하면 minimize 지점을 찾기가 힘들어 집니다.

![18](https://i.imgur.com/9l2aLxS.png)

exponential 자체가 상당히 계산하기 비싼 term 이기 때문에 forward/backward 시 계산이 오래 걸립니다. 

![19](https://i.imgur.com/8KdWRaS.png)

tanh(x)은 sigmoid function의 2번째 문제인 non-zero centered 문제만 해결하고 나머지 문제는 해결을 못하였습니다. 

![20](https://i.imgur.com/16yLbhY.png)

Relu는 sigmoid와 달리 exponential term이 없고 max function 하나만 사용하므로 sigmoid/tanh 보다 계산 시 6배 정도 빠르다고 알려져 있습니다. 
Relu에서는 gradient는 어떻게 될까요?

x = -10일 때는 gradient = 0 입니다.

x = 10 일 때에는 gradient = 1 입니다. 

x = 0 일 때에는 좌/우 극한이 다르므로 의미가 없습니다.

![21](https://i.imgur.com/LssEYqI.png)

- Leaky Relue : Relu에서는 Negative term에 대해서 모두 gradient가 0이 되지만 Leaky Relu 에서0이 되지 않도록 합니다. 

- PRelu :  Leaky Relu에서 사용된 negative 입력에 대한 gradient α 또한 학습을 하여 정하는 모델입니다.

![22](https://i.imgur.com/PF0NtEL.png)

ELU는 Relu에서 nagative 입력에 한하여 exponential term을 추가하는 형태 입니다. Relu의 장점과 더불어 gradient가 0으로 죽지 않고 output의 평균값이 zero에 가까워 지는 장점을 가지게 되지만 exponential term의 추가로 계산량이 증가하게 됩니다.

![23](https://i.imgur.com/OU9aTsD.png)

만약에 Weight를 전부 0으로 초기화 한다면 어떻게 될까요? 결과는 좋지 않습니다. 왜냐하면 back propagation 시 연산되는 결과 같이 모두 같아져 버리게 됩니다. 

![24](https://i.imgur.com/hETDcbB.png)

위는 weight initialize에 대한 cs231n의 실험한 코드 입니다.