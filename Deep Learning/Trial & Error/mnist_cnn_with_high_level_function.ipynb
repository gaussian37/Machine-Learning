import tensorflow as tf
import random
import matplotlib.pyplot as plt

from tensorflow.examples.tutorials.mnist import input_data

tf.set_random_seed(777)

mnist = input_data.read_data_sets("../data/", one_hot=True)

# hyper parameters
learning_rate = 0.001
training_epochs = 1
batch_size = 100

# input placeholder
X = tf.placeholder(tf.float32, shape=[None, 28*28])
X_img = tf.reshape(X, shape=[-1, 28, 28, 1])
Y = tf.placeholder(tf.float32, shape=[None, 10])

is_train = tf.placeholder(tf.bool)

# L1 input shape : (?, 28, 28, 1)
# L1 output shape : (?, 14, 14, 32)
conv1 = tf.layers.conv2d(inputs= X_img, filters=32, kernel_size=[3, 3], padding="SAME", activation=tf.nn.relu)
pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2, padding='SAME')
drop1 = tf.layers.dropout(inputs = pool1, rate = 0.3, training = is_train)

# L2 input shape : (?, 14, 14, 32)
# L2 output shape : (?, 7, 7, 64)
conv2 = tf.layers.conv2d(inputs = drop1, filters = 64, kernel_size=[3, 3], padding = "SAME", activation=tf.nn.relu)
pool2 = tf.layers.max_pooling2d(inputs = conv2, pool_size = [2, 2], strides = 2, padding = "SAME")
drop2 = tf.layers.dropout(inputs = pool2, rate = 0.3, training = is_train)

# L3 input shape : (?, 7, 7, 64)
# L3 output shape : (?, 4, 4, 128) => (?, 2048)
conv3 = tf.layers.conv2d(inputs = drop2, filters = 128, kernel_size = [3, 3], padding = "SAME", activation = tf.nn.relu)
pool3 = tf.layers.max_pooling2d(inputs = conv3, pool_size = [2, 2], strides = 2, padding = "SAME")
drop3 = tf.layers.dropout(inputs = pool3, rate = 0.3, training = is_train)
flat3 = tf.contrib.layers.flatten(drop3)

# L4 input shape : (?, 2048)
# L4 output shape : (?, 625)
dense4 = tf.layers.dense(inputs = flat3, units = 625, activation = tf.nn.relu)
drop4 = tf.layers.dropout(inputs = dense4, rate = 0.3, training = is_train)

# L5 input shape : (?, 625)
# L5 output shape : (?, 10)
logits = tf.layers.dense(inputs = drop4, units = 10)

# cost function & optimizer
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

# initialize
sess = tf.Session()
sess.run(tf.global_variables_initializer())

# train model
print("Learning started. It takes times")
for epoch in range(training_epochs):
    avg_cost = 0
    total_batch = int(mnist.train.num_examples / batch_size)
    
    for i in range(total_batch):
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
        feed_dict = {X:batch_xs, Y:batch_ys, is_train:True}
        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)
        avg_cost += c / total_batch
    print("Epoch : ", "%04d, " % (epoch+1), "%.9f" % (avg_cost))
print("Learning finished")

correct_predicition = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_predicition, tf.float32))
feed_dict = {X:mnist.test.images, Y:mnist.test.labels, is_train:False}
print("Accuracy : ", sess.run(accuracy, feed_dict=feed_dict))

# get one prediction
r = random.randint(0, mnist.test.num_examples -1)
print("Label : ", sess.run(tf.argmax(mnist.test.labels[[r]], 1)))
feed_dict = {X : mnist.train.images[[r]], is_train:False}
print("Prediction : ", sess.run(tf.argmax(logits, 1), feed_dict = feed_dict))

plt.imshow(mnist.test.images[[r]].reshape(28, 28), cmap='Greys', interpolation='nearest')
plt.show()