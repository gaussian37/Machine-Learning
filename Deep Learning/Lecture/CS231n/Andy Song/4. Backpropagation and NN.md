![1](https://i.imgur.com/YudOXEz.png)

지난 강의에서는 score function을 구하는 방법, SVM 또는 softmax를 이용하여 loss를 구하고 이렇게 구한 loss와 regularization을 이용하여 전체 loss를 구하는 방법에 대하여 알아보았습니다. 그리고 이를 통하여 알고자 하였던 ∇wL 즉, w의 변화에 따른 Loss의 변화에 대하여 알아보았습니다.

![2](https://i.imgur.com/DyjZHM2.jpg)

Optimization은 loss를 최소화 하는 W를 찾아가는 과정으로 배웠고 이를 parameter update 라고 하였습니다.

![3](https://i.imgur.com/xznAEYh.png)

gradient를 구하는 방법에는 Numerical gradient와 Analytic gradient가 있었습니다. 각각의 장단점에 대하여 공부하였고 실제적으로는 Analytic gradient를 사용하였습니다. 단, Numerical gradient를 이용하여 Analytic gradient의 값이 맞는지 확인하는 용도로 사용할 수 있었습니다.(Gradient check)

![4](https://i.imgur.com/rsDcSzQ.png)

그래서 이와 같이 Wx를 통하여 score를 얻고 score를 통하여 data loss를 얻은 후 Regularization와 더해 주어 total loss를 만듭니다. 이런 과정을 computational graph 라고 합니다. 하지만 복잡한 구조에서는 computational graph를 이용하기는 어렵습니다.

![5](https://i.imgur.com/KLICKOs.png)

위와 같이 복잡한 구조에서는 computational graph를 이용하여 loss를 구하는 것은 너무 버거운 일입니다. 그래서 이번 강의에서는 하나 하나의 모듈로 계산해 나가는 방식을 학습해 보도록 하겠습니다.

![6](https://i.imgur.com/hk4RLc6.png)

f(x, y, z) = (x+y)z 라고 하였을 때 임의의 값들을 이용하여 graph를 그려보았습니다. <br>
위의 graph 처럼 왼쪽 → 오른쪽으로 흘러가는 graph를 forward pass 또는 forward-propagation이라고 합니다.<br> 
여기서 우리가 구해야 하는 것은 gradient 입니다. 즉, input이 마지막 단에 어느 정도 영향을 미치는가를 구하고 싶은 것 입니다. <br>
따라서 gradient를 구하기 위해, + 연산에 매개변수 q를 두도록 하겠습니다.

![7](https://i.imgur.com/8lZ0hiF.png)

그림에서 보시다시피 매개변수 q를 도입하였고, f = qz로 표현할 수 있습니다. <br>
출력 단계의 f 부터 미분을 해주게 되면 각각 q와 z에 대하여 미분할 수 있습니다.<br>
∂f / ∂q = z, ∂f / ∂z = q임을 알 수 있습니다. <br>
q = x + y 이므로 ∂q / ∂x = 1, ∂q / ∂y = 1이 되는 것 또한 확인할 수 있습니다.

**우리가 원하는 것은 input 에 대한 마지막 단계에서의 영향력** 입니다. <br>
따라서 **입력단 x, y, z에 대한 출력단의 변화율인 ∂f/∂x, ∂f/∂y, ∂f/∂z 값이 필요**합니다.<br>
이를 위해서 backward pass 또는 back-propagation을 해야 합니다.  forward-propagation과 반대로 입력층 ← 출력층 으로 전달되는 값을 구해야 입력단에 대한 출력단의 변화율을 구할 수 있습니다.

![8](https://i.imgur.com/etgaCH9.png) 

먼저, 출력단 f에 대한 미분 즉, ∂f / ∂f = 1이 됩니다. 여기서 구분한 사항은 위쪽 녹색 값은 FP의 값이고 빨간색은 BP의 gradient 값입니다. 여기서 BP의 gradient는 각각의 노드 값들을 이용하여 f를 미분한 것입니다. 즉, q에서의 gradient는 ∂f / ∂q 가 되는 것이고 x에서의 gradient는 ∂f / ∂x 가 되는 것입니다.

![9](https://i.imgur.com/4XKm2hO.png)

∂f / ∂z = q를 이용하여 쉽게 gradient = 3을 구할 수 있습니다. <br>
이 때 ∂f / ∂z = 3의 의미를 이해해보면 **z의 값을 h만큼 변화하면 f의 값은 3*h 만큼 변화**한다는 뜻입니다. 즉, 입력의 3배만큼의 변화를 출력에서 가지게 된다는 뜻이고 그래서 변화율이 3이되게 됩니다. 

![10](https://i.imgur.com/Ia4JKMV.png)

이번에는 q에 대한 출력 값의 변화율을 보면 ∂f / ∂q = z = -4입니다. 이것 또한 q를 h만큼 변화시킨다면 f는 -4*h만큼 변화한다는 뜻입니다. 

![11](https://i.imgur.com/zLFRWpd.png)

이제 y에 대한 f의 변화율을 구해보려고 합니다. 하지만 y에 대한 f의 변화율을 바로 구하기는 어려워 chain rule을 이용하려고 합니다. 
결국 ∂f / ∂y = (∂f / ∂q) * (∂q / ∂y)로 표현할 수 있고, 앞에서 저희는 ∂f / ∂q의 값을 구하였습니다. 따라서 ∂f / ∂y = (-4) * (1)  = -4가 됩니다.
이 의미는 y가 h만큼 변화하였을 때 f는 -4*h만큼 변화한다는 뜻입니다. 그리고 q의 경우에는 y와 같은 gradient 값을 가지고 있습니다. 따라서 **y가 h만큼 변화하였을 때 q 또한 h만큼 변화**하게 됩니다.

![12](https://i.imgur.com/c6NUH3b.png)

∂f / ∂y를 구하는 chain rule의 ∂q / ∂y 는 직접적인 gradient 값이 되기 때문에 local gradient라고 하고 반대로 ∂f / ∂q 를 global gradient 라고 합니다. 

![13](https://i.imgur.com/7Rhr9Gu.png)

마지막으로 ∂f / ∂x 의 값은 ∂f / ∂y를 구할 때와 동일 합니다.
∂f / ∂x = (∂f / ∂q) * (∂q / ∂x) = -4 * 1 = -4 가 됩니다. 따라서 x의 h만큼 변화에 따른 f의 변화량은 -4*h 이고, q의 변화량은 h가 됩니다.

![14](https://i.imgur.com/K7Nk61N.png)

간단하게 gradient를 구하는 방법에 대하여 알아보았습니다. 여기서 반드시 알아야 하는 내용은 이 함수를 하나의 layer 또는 gate 로 표현하였고 FP(forward propagation) 과정 중이라 생각하면 FP 중에 바로 local gradient를 구할 수 있습니다. gradient를 구할 때, gradient = global gradient * local gradient 라고 학습하였습니다. 

![15](https://i.imgur.com/FLfHYg6.png)

다시 이전 슬라이드를 보면 FP 중에 local gradient를 구할 수 있었습니다. 예를 들어 q = x+y 에서 ∂q / ∂x = 1, ∂q / ∂y = 1을 구하였고 f = qz 에서 ∂f / ∂q = z , ∂f / ∂z = q를 구할 수 있었습니다. 

![16](https://i.imgur.com/YQzPu4x.png)

따라서 **핵심 내용은 local gradient는 FP 시 계산하여 메모리에 저장해 둔다는 것이 핵심**이 되겠습니다.

![17](https://i.imgur.com/aHJqGQa.png)

위 자료에서 global gradient는 ∂L / ∂z 가 되는데 이 값은 Back propagation을 하는 과정에서만 구할 수 있습니다.

![18](https://i.imgur.com/dSs1yDZ.png) 

즉, **정리하면 FP에서는 local gradient를 구하고 BP에서는 global gradient를 계산을 하여 최종적으로 BP에서 global gradient * local gradient 로 gradient를 얻을 수 있습니다. **따라서 위 자료를 기준으로 ∂L / ∂x = global gradient * local gradient = (∂L / ∂z) * (∂z / ∂x)가 되고 ∂L / ∂y = global gradient * local gradient = (∂L / ∂z) * (∂z / ∂y) 가 됩니다. 

![19](https://i.imgur.com/rnS84dz.png)

∂L / ∂x 와 ∂L / ∂y는 계속하여 BP를 통해 계속 전파 됩니다.<br>
**만약 BP 시 전파 되는 노드가 여러개라면, 예를 들어 z 한개가 아닌 z1, z2, z3, ... 가 f로 합쳐진다면 그 값들을 다 합쳐주면 됩니다.**

