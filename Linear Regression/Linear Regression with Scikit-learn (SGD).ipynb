{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Scikit-learn (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적인 Gradient Descent 식을 보면 다음과 같습니다.\n",
    "\n",
    "$\\theta _{j}:=\\theta_{j} - \\alpha\\frac{\\partial}{\\partial\\theta_{j}}J(\\theta_{0},\\theta_{1})$ <br><br>\n",
    "$\\frac{\\partial J}{\\partial w_{0}} = \\frac{1}{m} \\Sigma _{i=1}^{m}(w_{1}x^{i} + w_{0} - y^{(i)})$ <br><br>\n",
    "$\\frac{\\partial J}{\\partial w_{1}} = \\frac{1}{m} \\Sigma _{i=1}^{m}(w_{1}x^{i} + w_{0} - y^{(i)})x^{(i)}$\n",
    "\n",
    "위 식에서 사용할 때 수식 $\\Sigma _{i=1}^{m}$에서 알 수 있듯이 한 번에 모든 데이터를 이용하여 Gradient Descent를 수행하게 됩니다. 이렇게 한 번의 업데이트에 모든 데이터를 이용하는 것을 **Full-batch Gradient Descent** 라고 합니다. 한 번에 한 개의 데이터를 이용하여 gradient descent를 통한update 하는 것 보다 전체 데이터를 이용하여 update를 하는 것이 효과적입니다. \n",
    "\n",
    "용어를 정리하면, <br>\n",
    "- Gradient Descent : 1개의 데이터를 기준으로 미분. 즉, Singl Gradient Descent\n",
    "- Full-batch Gradient Descent : **모든 데이터 셋**으로 학습, 일반적인 GD라고 생각하면 됨\n",
    "<br>\n",
    "### ★ Full-batch Gradient Descent를 사용하는 경우 문제점 <br>\n",
    "- 메모리 문제가 발생. RAM에 데이터를 upload 한 후 처리를 하게 되는데, 대용량 데이터를 한번에 upload하면 메모리 초과 문제가 발생한다.\n",
    "- 계산 속도가 느려져 모델의 파라미터 업데이트가 느려짐\n",
    "- 모든 데이터를 사용하여 update를 하기 때문에 local minimum에 빠지게 되면 탈출하기 어려움\n",
    "\n",
    "![1](./nb_images/sgd_graph.jpg)\n",
    "\n",
    "local minimum에 빠졌을 때 전체 데이터를 계속 사용하면 계속 같은 데이터를 사용하기 때문에 local minimum에 계속 빠져있으나, 일부 데이터만 사용하면 update 값이 변경되어 local minimum에 빠져나올 수도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD (Stochastic Gradient Descent)\n",
    "- 원래 의미는 dataset에서 random 하게 training sample을 뽑은 후 학습할 때 사용\n",
    "- data를 넣기 전에 shuffle\n",
    "\n",
    "### Pseudo-code\n",
    "Procedure **SGD** <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;suffle(X) <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;for i in number of X do<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\theta_{j}:=\\theta_{j} - \\alpha(\\hat{y}^{(i)}-y^{(i)})x_{j}^{(i)}$ <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;end for<br>\n",
    "end Procedure\n",
    "\n",
    "- 데이터를 하나 하나 업데이트 하기 때문에 업데이트는 빠르고 빈번하게 할 수 있음.\n",
    "- Full-batch GD에 비하여 local minimum에서 잘 빠져나올 수 있음\n",
    "- 대용량 데이터 적용 시 전체 데이터 수행하는 데 시간이 너무 오래 걸릴 수 있음\n",
    "\n",
    "## Mini-batch Stochastic Gradient Descent\n",
    "\n",
    "- 한 번의 일정량의 데이터를 랜덤하게 뽑아서 학습\n",
    "- SGD와 Full-batch GD의 절충점\n",
    "- 가장 일반적으로 많이 쓰이는 기법\n",
    "\n",
    "### Epoch\n",
    "- Full batch 를 n번 실행하면 n epoch\n",
    "- 전체 데이터가 Training 데이터에 들어갈 때 카운팅\n",
    "\n",
    "### Batch size\n",
    "- 한 번에 학습되는 데이터의 갯수\n",
    "- 일반적으로 $2^N$ 크기의 Batch size를 사용 (CPU/GPU 원리 상 $2^N$ 단위로 동작함)\n",
    "\n",
    "ex) 총 5,120개의 Training data에 512 batch-size면 몇 번을 학습해야 1 epoch이 되는가? 10번\n",
    "\n",
    "### Pseudo-code\n",
    "Procedure **Mini-batch SGD** <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;suffle(X) <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;BS ← Batch Size <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;NB ← Number of Batches <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;NB ← len(X) // BS <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;for i in number of X do<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\theta_{j}:=\\theta_{j} - \\alpha\\Sigma_{k=i*BS}^{(i+1)*BS}(\\hat{y}^{(k)}-y^{(k)})x_{j}^{(k)}$ <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;end for<br>\n",
    "end Procedure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
